services: 
  llama-cpp-python-rocm:
    build:
      context: .

    # for debugging
    #entrypoint: tail
    #command: -f /dev/null

    volumes:
      - ./data/hugging_face_models:/root/.cache/huggingface/hub
      - ./data/static_models:/models

    # See compatbility matrix: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html
    # you need to compile this for your architecture
    # for GCN 5th gen based GPUs and APUs HSA_OVERRIDE_GFX_VERSION=9.0.0
    # for RDNA 1 based GPUs and APUs HSA_OVERRIDE_GFX_VERSION=10.1.0
    # for RDNA 2 based GPUs and APUs HSA_OVERRIDE_GFX_VERSION=10.3.0
    # for RDNA 3 based GPUs and APUs HSA_OVERRIDE_GFX_VERSION=11.0.0
    environment:
      #- HSA_OVERRIDE_GFX_VERSION=9.0.0
      - PYTORCH_ROCM_ARCH="gfx906"
      - HIP_VISIBLE_DEVICES=0
      
      # https://llama-cpp-python.readthedocs.io/en/latest/server/#llama_cpp.server.settings.ServerSettings
      # https://github.com/MeetKai/functionary?tab=readme-ov-file#llamacpp-inference
      # no functionary 2.5 or 3.2 yet :(
      # see also: https://github.com/abetlen/llama-cpp-python/pull/1509
      - MODEL=functionary-small-v2.4.Q4_0.gguf
      - CHAT_FORMAT=functionary-v2 # v3 is not available in python wrapper but has references in llamacpp

      - HF_MODEL_REPO_ID=meetkai/functionary-small-v2.4-GGUF
      - HF_PRETRAINED_MODEL_NAME_OR_PATH=meetkai/functionary-small-v2.4-GGUF
      #- HF_TOKENIZER_CONFIG_PATH=tokenizer_config.json

      - USE_MLOCK=0
      - USE_MMAP=1
      - N_GPU_LAYERS=-1
      - N_CTX=4096

    devices:
      - /dev/dri:/dev/dri
      - /dev/kfd:/dev/kfd # ROCm Kernel driver

    ports:
      - 8000:8000  

    security_opt:
      - seccomp:unconfined
    group_add:
      - video
